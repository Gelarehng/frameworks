{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:FRNN is available\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "import yaml\n",
    "from itertools import chain, product, combinations\n",
    "import torch\n",
    "\n",
    "from time import time as tt\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"../../../\")\n",
    "from gnn4itk_cf.stages.data_reading.models.trackml_utils import *\n",
    "\n",
    "from gnn4itk_cf.stages.data_reading.data_reading_stage import EventReader\n",
    "from gnn4itk_cf.stages.data_reading.models.trackml_reader import TrackMLReader\n",
    "\n",
    "from gnn4itk_cf.stages.graph_construction.models.metric_learning import MetricLearning\n",
    "from gnn4itk_cf.stages.edge_classifier.models.filter import Filter\n",
    "from gnn4itk_cf.stages.edge_classifier import InteractionGNN\n",
    "\n",
    "from gnn4itk_cf.stages.graph_construction.utils import handle_weighting\n",
    "from gnn4itk_cf.stages.graph_construction.models.utils import graph_intersection, build_edges\n",
    "from gnn4itk_cf.stages.graph_construction.utils import *\n",
    "\n",
    "from gnn4itk_cf.stages.track_building import utils \n",
    "from torch_geometric.utils import to_scipy_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"examples/Example_3/metric_learning_train.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "model_ML = MetricLearning(config)\n",
    "with open(\"examples/Example_3/filter_train.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "model_filter = Filter(config)\n",
    "with open(\"examples/Example_3/gnn_train.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "model_gnn = InteractionGNN(config)\n",
    "config = yaml.safe_load(open(\"examples/Example_3/track_building_eval.yaml\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 80 training events, 10 validation events and 10 testing events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/gnn4itk_cf/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [pid] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/workspace/gnn4itk_cf/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [n_hits] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/workspace/gnn4itk_cf/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [primary] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/workspace/gnn4itk_cf/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [pdg_id] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/workspace/gnn4itk_cf/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [ghost] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/workspace/gnn4itk_cf/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [shared] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/workspace/gnn4itk_cf/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [module_id] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/workspace/gnn4itk_cf/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [region_id] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/workspace/gnn4itk_cf/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [hit_id] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/workspace/gnn4itk_cf/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [particle_id] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/workspace/gnn4itk_cf/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [nhits] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/workspace/gnn4itk_cf/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [pdgId] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/workspace/gnn4itk_cf/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [region] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/workspace/gnn4itk_cf/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [pt] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/workspace/gnn4itk_cf/stages/edge_classifier/edge_classifier_stage.py:95: UserWarning: Failed to define figures of merit, due to logger unavailable\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining figures of merit\n",
      "Defining figures of merit\n"
     ]
    }
   ],
   "source": [
    "model_ML.setup(stage=\"predict\")\n",
    "dataloaders = model_ML.predict_dataloader()\n",
    "model_filter.setup('predict')\n",
    "model_gnn.setup('predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reconstruction_df(graph):\n",
    "    \"\"\"Load the reconstructed tracks from a file.\"\"\"\n",
    "    pids = torch.zeros(graph.hit_id.shape[0], dtype=torch.int64)\n",
    "    pids[graph.track_edges[0]] = graph.particle_id\n",
    "    pids[graph.track_edges[1]] = graph.particle_id\n",
    "\n",
    "    return pd.DataFrame({\"hit_id\": graph.hit_id, \"track_id\": graph.labels, \"particle_id\": pids})\n",
    "\n",
    "def load_particles_df(graph):\n",
    "    \"\"\"Load the particles from a file.\"\"\"\n",
    "    # Get the particle dataframe\n",
    "    particles_df = pd.DataFrame({\"particle_id\": graph.particle_id, \"pt\": graph.pt})\n",
    "\n",
    "    # Reduce to only unique particle_ids\n",
    "    particles_df = particles_df.drop_duplicates(subset=['particle_id'])\n",
    "\n",
    "    return particles_df\n",
    "\n",
    "def get_matching_df(reconstruction_df, min_track_length=1, min_particle_length=1):\n",
    "    \n",
    "    # Get track lengths\n",
    "    candidate_lengths = reconstruction_df.track_id.value_counts(sort=False)\\\n",
    "        .reset_index().rename(\n",
    "            columns={\"index\":\"track_id\", \"track_id\": \"n_reco_hits\"})\n",
    "\n",
    "    # Get true track lengths\n",
    "    particle_lengths = reconstruction_df.drop_duplicates(subset=['hit_id']).particle_id.value_counts(sort=False)\\\n",
    "        .reset_index().rename(\n",
    "            columns={\"index\":\"particle_id\", \"particle_id\": \"n_true_hits\"})\n",
    "\n",
    "    spacepoint_matching = reconstruction_df.groupby(['track_id', 'particle_id']).size()\\\n",
    "        .reset_index().rename(columns={0:\"n_shared\"})\n",
    "\n",
    "    spacepoint_matching = spacepoint_matching.merge(candidate_lengths, on=['track_id'], how='left')\n",
    "    spacepoint_matching = spacepoint_matching.merge(particle_lengths, on=['particle_id'], how='left')\n",
    "    # spacepoint_matching = spacepoint_matching.merge(particles_df, on=['particle_id'], how='left')\n",
    "\n",
    "    # Filter out tracks with too few shared spacepoints\n",
    "    spacepoint_matching[\"is_matchable\"] = spacepoint_matching.n_reco_hits >= min_track_length\n",
    "    spacepoint_matching[\"is_reconstructable\"] = spacepoint_matching.n_true_hits >= min_particle_length\n",
    "\n",
    "    return spacepoint_matching\n",
    "\n",
    "def calculate_matching_fraction(spacepoint_matching_df):\n",
    "    spacepoint_matching_df = spacepoint_matching_df.assign(\n",
    "        purity_reco=np.true_divide(spacepoint_matching_df.n_shared, spacepoint_matching_df.n_reco_hits))\n",
    "    spacepoint_matching_df = spacepoint_matching_df.assign(\n",
    "        eff_true = np.true_divide(spacepoint_matching_df.n_shared, spacepoint_matching_df.n_true_hits))\n",
    "\n",
    "    return spacepoint_matching_df\n",
    "\n",
    "def evaluate_labelled_graph(graph, matching_fraction=0.5, matching_style=\"ATLAS\", min_track_length=1, min_particle_length=1):\n",
    "\n",
    "    if matching_fraction < 0.5:\n",
    "        raise ValueError(\"Matching fraction must be >= 0.5\")\n",
    "\n",
    "    if matching_fraction == 0.5:\n",
    "        # Add a tiny bit of noise to the matching fraction to avoid double-matched tracks\n",
    "        matching_fraction += 0.00001\n",
    "\n",
    "    # Load the labelled graphs as reconstructed dataframes\n",
    "    reconstruction_df = load_reconstruction_df(graph)\n",
    "    particles_df = load_particles_df(graph)\n",
    "\n",
    "    # Get matching dataframe\n",
    "    matching_df = get_matching_df(reconstruction_df, particles_df, min_track_length=min_track_length, min_particle_length=min_particle_length) \n",
    "    matching_df[\"event_id\"] = int(graph.event_id)\n",
    "\n",
    "    # calculate matching fraction\n",
    "    matching_df = calculate_matching_fraction(matching_df)\n",
    "\n",
    "    # Run matching depending on the matching style\n",
    "    if matching_style == \"ATLAS\":\n",
    "        matching_df[\"is_matched\"] = matching_df[\"is_reconstructed\"] = matching_df.purity_reco >= matching_fraction\n",
    "    elif matching_style == \"one_way\":\n",
    "        matching_df[\"is_matched\"] = matching_df.purity_reco >= matching_fraction\n",
    "        matching_df[\"is_reconstructed\"] = matching_df.eff_true >= matching_fraction\n",
    "    elif matching_style == \"two_way\":\n",
    "        matching_df[\"is_matched\"] = matching_df[\"is_reconstructed\"] = (matching_df.purity_reco >= matching_fraction) & (matching_df.eff_true >= matching_fraction)\n",
    "\n",
    "    return matching_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_labelled_graphs(graphset, config):\n",
    "    all_y_truth, all_pt  = [], []\n",
    "    evaluated_events = [\n",
    "        utils.evaluate_labelled_graph(\n",
    "            event,\n",
    "            matching_fraction=config[\"matching_fraction\"],\n",
    "            matching_style=config[\"matching_style\"],\n",
    "            min_track_length=config[\"min_track_length\"],\n",
    "            min_particle_length=config[\"min_particle_length\"],\n",
    "        )\n",
    "        for event in tqdm(graphset)\n",
    "    ]\n",
    "    evaluated_events = pd.concat(evaluated_events)\n",
    "\n",
    "    particles = evaluated_events[evaluated_events[\"is_reconstructable\"]]\n",
    "    reconstructed_particles = particles[particles[\"is_reconstructed\"] & particles[\"is_matchable\"]]\n",
    "    tracks = evaluated_events[evaluated_events[\"is_matchable\"]]\n",
    "    matched_tracks = tracks[tracks[\"is_matched\"]]\n",
    "\n",
    "    n_particles = len(particles.drop_duplicates(subset=['event_id', 'particle_id']))\n",
    "    n_reconstructed_particles = len(reconstructed_particles.drop_duplicates(subset=['event_id', 'particle_id']))\n",
    "\n",
    "    n_tracks = len(tracks.drop_duplicates(subset=['event_id', 'track_id']))\n",
    "    n_matched_tracks = len(matched_tracks.drop_duplicates(subset=['event_id', 'track_id']))\n",
    "\n",
    "    n_dup_reconstructed_particles = len(reconstructed_particles) - n_reconstructed_particles\n",
    "\n",
    "    print(f\"Number of reconstructed particles: {n_reconstructed_particles}\")\n",
    "    print(f\"Number of particles: {n_particles}\")\n",
    "    print(f\"Number of matched tracks: {n_matched_tracks}\")\n",
    "    print(f\"Number of tracks: {n_tracks}\")\n",
    "    print(f\"Number of duplicate reconstructed particles: {n_dup_reconstructed_particles}\")   \n",
    "\n",
    "    # Plot the results across pT and eta\n",
    "    eff = n_reconstructed_particles / n_particles\n",
    "    fake_rate = 1 - (n_matched_tracks / n_tracks)\n",
    "    dup_rate = n_dup_reconstructed_particles / n_reconstructed_particles\n",
    "\n",
    "    logging.info(f\"Efficiency: {eff:.3f}\")\n",
    "    logging.info(f\"Fake rate: {fake_rate:.3f}\")\n",
    "    logging.info(f\"Duplication rate: {dup_rate:.3f}\")\n",
    "    print(f\"Efficiency: {eff:.3f}\")\n",
    "    print(f\"Fake rate: {fake_rate:.3f}\")\n",
    "    print(f\"Duplication rate: {dup_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(cell_val=[15682], geta=[15682], weight=[15682], region=[15682], lx=[15682], lphi=[15682], module_index=[15682], x=[15682], r=[15682], gphi=[15682], hit_id=[15682], lz=[15682], z=[15682], cell_count=[15682], phi=[15682], y=[15682], ly=[15682], leta=[15682], eta=[15682], track_edges=[2, 14278], particle_id=[14278], radius=[14278], nhits=[14278], pt=[14278], config=[2], event_id=[1], num_nodes=15682, batch=[15682], ptr=[2], edge_index=[2, 111209], scores=[111209], labels=[15682])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 17.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reconstructed particles: 1\n",
      "Number of particles: 1386\n",
      "Number of matched tracks: 1\n",
      "Number of tracks: 98\n",
      "Number of duplicate reconstructed particles: 0\n",
      "Efficiency: 0.001\n",
      "Fake rate: 0.990\n",
      "Duplication rate: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(cell_val=[14296], geta=[14296], weight=[14296], region=[14296], lx=[14296], lphi=[14296], module_index=[14296], x=[14296], r=[14296], gphi=[14296], hit_id=[14296], lz=[14296], z=[14296], cell_count=[14296], phi=[14296], y=[14296], ly=[14296], leta=[14296], eta=[14296], track_edges=[2, 13029], particle_id=[13029], radius=[13029], nhits=[13029], pt=[13029], config=[2], event_id=[1], num_nodes=14296, batch=[14296], ptr=[2], edge_index=[2, 102476], scores=[102476], labels=[14296])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 34.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reconstructed particles: 2\n",
      "Number of particles: 1244\n",
      "Number of matched tracks: 2\n",
      "Number of tracks: 98\n",
      "Number of duplicate reconstructed particles: 0\n",
      "Efficiency: 0.002\n",
      "Fake rate: 0.980\n",
      "Duplication rate: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(cell_val=[14783], geta=[14783], weight=[14783], region=[14783], lx=[14783], lphi=[14783], module_index=[14783], x=[14783], r=[14783], gphi=[14783], hit_id=[14783], lz=[14783], z=[14783], cell_count=[14783], phi=[14783], y=[14783], ly=[14783], leta=[14783], eta=[14783], track_edges=[2, 13474], particle_id=[13474], radius=[13474], nhits=[13474], pt=[13474], config=[2], event_id=[1], num_nodes=14783, batch=[14783], ptr=[2], edge_index=[2, 106243], scores=[106243], labels=[14783])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 35.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reconstructed particles: 2\n",
      "Number of particles: 1288\n",
      "Number of matched tracks: 2\n",
      "Number of tracks: 102\n",
      "Number of duplicate reconstructed particles: 0\n",
      "Efficiency: 0.002\n",
      "Fake rate: 0.980\n",
      "Duplication rate: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(cell_val=[14890], geta=[14890], weight=[14890], region=[14890], lx=[14890], lphi=[14890], module_index=[14890], x=[14890], r=[14890], gphi=[14890], hit_id=[14890], lz=[14890], z=[14890], cell_count=[14890], phi=[14890], y=[14890], ly=[14890], leta=[14890], eta=[14890], track_edges=[2, 13576], particle_id=[13576], radius=[13576], nhits=[13576], pt=[13576], config=[2], event_id=[1], num_nodes=14890, batch=[14890], ptr=[2], edge_index=[2, 106951], scores=[106951], labels=[14890])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 32.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reconstructed particles: 1\n",
      "Number of particles: 1292\n",
      "Number of matched tracks: 1\n",
      "Number of tracks: 93\n",
      "Number of duplicate reconstructed particles: 0\n",
      "Efficiency: 0.001\n",
      "Fake rate: 0.989\n",
      "Duplication rate: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(cell_val=[13415], geta=[13415], weight=[13415], region=[13415], lx=[13415], lphi=[13415], module_index=[13415], x=[13415], r=[13415], gphi=[13415], hit_id=[13415], lz=[13415], z=[13415], cell_count=[13415], phi=[13415], y=[13415], ly=[13415], leta=[13415], eta=[13415], track_edges=[2, 12206], particle_id=[12206], radius=[12206], nhits=[12206], pt=[12206], config=[2], event_id=[1], num_nodes=13415, batch=[13415], ptr=[2], edge_index=[2, 94924], scores=[94924], labels=[13415])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 36.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reconstructed particles: 3\n",
      "Number of particles: 1190\n",
      "Number of matched tracks: 3\n",
      "Number of tracks: 92\n",
      "Number of duplicate reconstructed particles: 0\n",
      "Efficiency: 0.003\n",
      "Fake rate: 0.967\n",
      "Duplication rate: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(cell_val=[11113], geta=[11113], weight=[11113], region=[11113], lx=[11113], lphi=[11113], module_index=[11113], x=[11113], r=[11113], gphi=[11113], hit_id=[11113], lz=[11113], z=[11113], cell_count=[11113], phi=[11113], y=[11113], ly=[11113], leta=[11113], eta=[11113], track_edges=[2, 10146], particle_id=[10146], radius=[10146], nhits=[10146], pt=[10146], config=[2], event_id=[1], num_nodes=11113, batch=[11113], ptr=[2], edge_index=[2, 78081], scores=[78081], labels=[11113])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 38.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reconstructed particles: 5\n",
      "Number of particles: 955\n",
      "Number of matched tracks: 5\n",
      "Number of tracks: 100\n",
      "Number of duplicate reconstructed particles: 0\n",
      "Efficiency: 0.005\n",
      "Fake rate: 0.950\n",
      "Duplication rate: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(cell_val=[7859], geta=[7859], weight=[7859], region=[7859], lx=[7859], lphi=[7859], module_index=[7859], x=[7859], r=[7859], gphi=[7859], hit_id=[7859], lz=[7859], z=[7859], cell_count=[7859], phi=[7859], y=[7859], ly=[7859], leta=[7859], eta=[7859], track_edges=[2, 7154], particle_id=[7154], radius=[7154], nhits=[7154], pt=[7154], config=[2], event_id=[1], num_nodes=7859, batch=[7859], ptr=[2], edge_index=[2, 56255], scores=[56255], labels=[7859])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 46.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reconstructed particles: 6\n",
      "Number of particles: 692\n",
      "Number of matched tracks: 6\n",
      "Number of tracks: 83\n",
      "Number of duplicate reconstructed particles: 0\n",
      "Efficiency: 0.009\n",
      "Fake rate: 0.928\n",
      "Duplication rate: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(cell_val=[12086], geta=[12086], weight=[12086], region=[12086], lx=[12086], lphi=[12086], module_index=[12086], x=[12086], r=[12086], gphi=[12086], hit_id=[12086], lz=[12086], z=[12086], cell_count=[12086], phi=[12086], y=[12086], ly=[12086], leta=[12086], eta=[12086], track_edges=[2, 11006], particle_id=[11006], radius=[11006], nhits=[11006], pt=[11006], config=[2], event_id=[1], num_nodes=12086, batch=[12086], ptr=[2], edge_index=[2, 85080], scores=[85080], labels=[12086])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 42.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reconstructed particles: 3\n",
      "Number of particles: 1069\n",
      "Number of matched tracks: 3\n",
      "Number of tracks: 101\n",
      "Number of duplicate reconstructed particles: 0\n",
      "Efficiency: 0.003\n",
      "Fake rate: 0.970\n",
      "Duplication rate: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(cell_val=[14300], geta=[14300], weight=[14300], region=[14300], lx=[14300], lphi=[14300], module_index=[14300], x=[14300], r=[14300], gphi=[14300], hit_id=[14300], lz=[14300], z=[14300], cell_count=[14300], phi=[14300], y=[14300], ly=[14300], leta=[14300], eta=[14300], track_edges=[2, 13026], particle_id=[13026], radius=[13026], nhits=[13026], pt=[13026], config=[2], event_id=[1], num_nodes=14300, batch=[14300], ptr=[2], edge_index=[2, 101029], scores=[101029], labels=[14300])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 37.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reconstructed particles: 3\n",
      "Number of particles: 1248\n",
      "Number of matched tracks: 3\n",
      "Number of tracks: 100\n",
      "Number of duplicate reconstructed particles: 0\n",
      "Efficiency: 0.002\n",
      "Fake rate: 0.970\n",
      "Duplication rate: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(cell_val=[11896], geta=[11896], weight=[11896], region=[11896], lx=[11896], lphi=[11896], module_index=[11896], x=[11896], r=[11896], gphi=[11896], hit_id=[11896], lz=[11896], z=[11896], cell_count=[11896], phi=[11896], y=[11896], ly=[11896], leta=[11896], eta=[11896], track_edges=[2, 10851], particle_id=[10851], radius=[10851], nhits=[10851], pt=[10851], config=[2], event_id=[1], num_nodes=11896, batch=[11896], ptr=[2], edge_index=[2, 84296], scores=[84296], labels=[11896])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 39.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reconstructed particles: 2\n",
      "Number of particles: 1030\n",
      "Number of matched tracks: 2\n",
      "Number of tracks: 98\n",
      "Number of duplicate reconstructed particles: 0\n",
      "Efficiency: 0.002\n",
      "Fake rate: 0.980\n",
      "Duplication rate: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device ='cuda'\n",
    "model_ML = model_ML.to(\"cuda\")\n",
    "model_filter = model_filter.to(\"cuda\")\n",
    "model_gnn = model_gnn.to(\"cuda\")\n",
    "for batch in dataloaders[2]:\n",
    "    batch = batch.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        if device == 'cuda':\n",
    "            with torch.cuda.amp.autocast():\n",
    "                embedding = model_ML.apply_embedding(batch)\n",
    "     \n",
    "    batch.edge_index = build_edges(\n",
    "        query=embedding, database=embedding, indices=None, r_max=0.1, k_max=10, backend=\"FRNN\"\n",
    "    )\n",
    "    R = batch.r**2 + batch.z**2\n",
    "    flip_edge_mask = R[batch.edge_index[0]] > R[batch.edge_index[1]]\n",
    "    batch.edge_index[:, flip_edge_mask] = batch.edge_index[:, flip_edge_mask].flip(0)\n",
    "    with torch.no_grad():\n",
    "        if device == 'cuda':\n",
    "            with torch.cuda.amp.autocast():\n",
    "                out = model_filter(batch)   \n",
    "    preds = torch.sigmoid(out)\n",
    "    batch.edge_index = batch.edge_index[:, preds > model_filter.hparams['edge_cut']]\n",
    "    with torch.no_grad():\n",
    "        if device == 'cuda':\n",
    "            with torch.cuda.amp.autocast():\n",
    "                out = model_gnn(batch)\n",
    "    batch.scores = torch.sigmoid(out)\n",
    "\n",
    "    edge_mask = batch.scores > model_gnn.hparams['edge_cut'] \n",
    "    # Get number of nodes\n",
    "    if hasattr(batch, \"num_nodes\"):\n",
    "        num_nodes = batch.num_nodes\n",
    "    elif hasattr(batch, \"x\"):\n",
    "        num_nodes = batch.x.size(0)\n",
    "    elif hasattr(batch, \"x_x\"):\n",
    "        num_nodes = batch.x_x.size(0)\n",
    "    else:\n",
    "        num_nodes = batch.edge_index.max().item() + 1\n",
    "    # Convert to sparse scipy array\n",
    "    sparse_edges = to_scipy_sparse_matrix(\n",
    "        batch.edge_index[:, edge_mask], num_nodes=num_nodes\n",
    "    )\n",
    "    # Run connected components\n",
    "    candidate_labels = sps.csgraph.connected_components(\n",
    "        sparse_edges, directed=False, return_labels=True\n",
    "    )\n",
    "    batch.labels = torch.from_numpy(candidate_labels[1]).long()\n",
    "    \n",
    "    batch.config.append(config)\n",
    "    print(batch)\n",
    "    evaluate_labelled_graphs([batch.to('cpu')], config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: Data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/workspace/acorn/notebooks/inferenceExample_3.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f676f6f66795f64617277696e227d/workspace/acorn/notebooks/inferenceExample_3.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m export_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(model_filter, sample,\u001b[39m\"\u001b[39;49m\u001b[39mfilter.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    191\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    209\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     _export(\n\u001b[1;32m    517\u001b[0m         model,\n\u001b[1;32m    518\u001b[0m         args,\n\u001b[1;32m    519\u001b[0m         f,\n\u001b[1;32m    520\u001b[0m         export_params,\n\u001b[1;32m    521\u001b[0m         verbose,\n\u001b[1;32m    522\u001b[0m         training,\n\u001b[1;32m    523\u001b[0m         input_names,\n\u001b[1;32m    524\u001b[0m         output_names,\n\u001b[1;32m    525\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[1;32m    526\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m    527\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m    528\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m    529\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m    530\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[1;32m    531\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[1;32m    532\u001b[0m         autograd_inlining\u001b[39m=\u001b[39;49mautograd_inlining,\n\u001b[1;32m    533\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:1582\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1580\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1582\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[1;32m   1583\u001b[0m     model,\n\u001b[1;32m   1584\u001b[0m     args,\n\u001b[1;32m   1585\u001b[0m     verbose,\n\u001b[1;32m   1586\u001b[0m     input_names,\n\u001b[1;32m   1587\u001b[0m     output_names,\n\u001b[1;32m   1588\u001b[0m     operator_export_type,\n\u001b[1;32m   1589\u001b[0m     val_do_constant_folding,\n\u001b[1;32m   1590\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m   1591\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1592\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m   1593\u001b[0m )\n\u001b[1;32m   1595\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[1;32m   1597\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1598\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:1135\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[1;32m   1134\u001b[0m model \u001b[39m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m-> 1135\u001b[0m graph, params, torch_out, module \u001b[39m=\u001b[39m _create_jit_graph(model, args)\n\u001b[1;32m   1136\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1138\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:1011\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     graph \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[1;32m   1007\u001b[0m         graph, flattened_args, param_count_list, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     )\n\u001b[1;32m   1009\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, params, torch_out, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1011\u001b[0m graph, torch_out \u001b[39m=\u001b[39m _trace_and_get_graph_from_model(model, args)\n\u001b[1;32m   1012\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m   1013\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:915\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    913\u001b[0m prev_autocast_cache_enabled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    914\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 915\u001b[0m trace_graph, torch_out, inputs_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_get_trace_graph(\n\u001b[1;32m    916\u001b[0m     model,\n\u001b[1;32m    917\u001b[0m     args,\n\u001b[1;32m    918\u001b[0m     strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    919\u001b[0m     _force_outplace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    920\u001b[0m     _return_inputs_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    921\u001b[0m )\n\u001b[1;32m    922\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[1;32m    924\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py:1287\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m   1286\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m-> 1287\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(\n\u001b[1;32m   1288\u001b[0m     f, strict, _force_outplace, return_inputs, _return_inputs_states\n\u001b[1;32m   1289\u001b[0m )(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1290\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py:100\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 100\u001b[0m     in_vars, in_desc \u001b[39m=\u001b[39m _flatten(args)\n\u001b[1;32m    101\u001b[0m     \u001b[39m# NOTE: use full state, because we need it for BatchNorm export\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[39m# This differs from the compiler path, which doesn't support it at the moment.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     module_state \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(_unique_state_dict(\u001b[39mself\u001b[39m, keep_vars\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mvalues())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: Data"
     ]
    }
   ],
   "source": [
    "export_output = torch.onnx.export(model_filter, sample,\"filter.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
